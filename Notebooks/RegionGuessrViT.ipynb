{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XBNVItUWqgMz"},"outputs":[],"source":["!pip install -qq -U tensorflow_addons"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Pu8akgygWKQ"},"outputs":[],"source":["import math\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_addons as tfa\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import layers\n","import pickle\n","import cv2\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGgDYlhKgi08"},"outputs":[],"source":["#Mount google drive to colab\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kS8hm-56j5-5"},"outputs":[],"source":["model_path = \"./gdrive/MyDrive/models/region_guessr_vit_mdl_wts.hdf5\" \n","images_path = \"./gdrive/MyDrive/lumen/data/\"\n","train_annotations_path = \"./gdrive/MyDrive/lumen/train_annotations.csv\"\n","validation_annotations_path = \"./gdrive/MyDrive/lumen/validation_annotations.csv\"\n","test_annotations_path = \"./gdrive/MyDrive/lumen/test_annotations.csv\"\n","# train_records_path = \"./gdrive/MyDrive/tf_records/train\"\n","# validation_records_path = \"./gdrive/MyDrive/tf_records/validation\"\n","regions_to_labels = {\"Istra\": 0, \"Lika\": 1, \"Dalmacija\": 2, \"Sjever\": 3, \"Slavonija\": 4, \"Sredisnja\": 5}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsU6NSPDhPrn"},"outputs":[],"source":["##Some code is taken from https://keras.io/examples/vision/vit_small_ds/ 10.6.2022.\n","NUM_CLASSES = 2\n","INPUT_SHAPE = (128, 128, 3)\n","\n","# DATA\n","BUFFER_SIZE = 512\n","BATCH_SIZE = 128\n","\n","# AUGMENTATION\n","IMAGE_SIZE = 128\n","PATCH_SIZE = 8\n","NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n","\n","# OPTIMIZER\n","LEARNING_RATE = 0.001\n","WEIGHT_DECAY = 0.0001\n","\n","# TRAINING\n","EPOCHS = 50\n","\n","# ARCHITECTURE\n","LAYER_NORM_EPS = 1e-6\n","TRANSFORMER_LAYERS = 8\n","PROJECTION_DIM = 128\n","NUM_HEADS = 4\n","TRANSFORMER_UNITS = [\n","    PROJECTION_DIM * 2,\n","    PROJECTION_DIM,\n","]\n","MLP_HEAD_UNITS = [2048, 1024]"]},{"cell_type":"code","source":["train_records_path = \"./gdrive/MyDrive/tf_records/regression_train\"\n","validation_records_path = \"./gdrive/MyDrive/tf_records/regression_validation\""],"metadata":{"id":"C4HXPwSAF5h4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CID2AQA9Hw43"},"outputs":[],"source":["TRAIN_FILENAMES = tf.io.gfile.glob(train_records_path + \"/*.tfrecords\")\n","print(train_records_path + \"/train*.tfrec\")\n","print(\"Train TFRecord Files:\", len(TRAIN_FILENAMES))\n","\n","VAL_FILENAMES = tf.io.gfile.glob(validation_records_path + \"/*.tfrecords\")\n","print(validation_records_path + \"/validation*.tfrec\")\n","print(\"Train TFRecord Files:\", len(VAL_FILENAMES))\n","\n","AUTOTUNE = tf.data.AUTOTUNE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yb21o2WxINns"},"outputs":[],"source":["def parse_tfr_element(element):\n","  data = {\n","      'raw_image' : tf.io.FixedLenFeature([], tf.string),\n","      'x_coord' : tf.io.FixedLenFeature([], tf.float32),\n","      'y_coord' : tf.io.FixedLenFeature([], tf.float32)\n","    }\n","\n","    \n","  content = tf.io.parse_single_example(element, data)\n","  \n","  x = content['x_coord']\n","  y = content['y_coord']\n","  raw_image = content['raw_image']\n","  \n","  \n","  feature = tf.io.parse_tensor(raw_image, out_type=tf.uint8)\n","  feature = tf.reshape(feature, shape=[128, 128,3])\n","  return (feature, [x, y])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pM2SgREpHu1k"},"outputs":[],"source":["def decode_image(image):\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = tf.cast(image, tf.float32)\n","    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jTW0V9MIHse2"},"outputs":[],"source":["def read_tfrecord(example, labeled):\n","    tfrecord_format = (\n","        {\n","          'raw_image' : tf.io.FixedLenFeature([], tf.string),\n","          'x_coord' : tf.io.FixedLenFeature([], tf.float32),\n","          'y_coord' : tf.io.FixedLenFeature([], tf.float32)\n","        }\n","        if labeled\n","        else {\"raw_image\": tf.io.FixedLenFeature([], tf.string),}\n","    )\n","    example = tf.io.parse_single_example(example, tfrecord_format)\n","    image = decode_image(example[\"raw_image\"])\n","    if labeled:\n","        x = tf.cast(example[\"x\"], tf.float32)\n","        y = tf.cast(example[\"y\"], tf.float32)\n","        return image, [x, y]\n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hdrc66kLHp2N"},"outputs":[],"source":["def load_dataset(filenames, labeled=True):\n","    ignore_order = tf.data.Options()\n","    ignore_order.experimental_deterministic = False  # disable order, increase speed\n","    dataset = tf.data.TFRecordDataset(\n","        filenames\n","    )  # automatically interleaves reads from multiple files\n","    dataset = dataset.with_options(\n","        ignore_order\n","    )  # uses data as soon as it streams in, rather than in its original order\n","    dataset = dataset.map(\n","        parse_tfr_element, num_parallel_calls=AUTOTUNE\n","    )\n","    # returns a dataset of (image, label) pairs if labeled=True or just images if labeled=False\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTXgUMdoHm5V"},"outputs":[],"source":["def get_dataset(filenames, labeled=True):\n","    dataset = load_dataset(filenames, labeled=labeled)\n","    dataset = dataset.shuffle(2048)\n","    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n","    dataset = dataset.batch(BATCH_SIZE)\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLTV5KJv6POn"},"outputs":[],"source":["train_dataset = get_dataset(TRAIN_FILENAMES)\n","validation_dataset = get_dataset(VAL_FILENAMES)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Yiv--BnhiZO"},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.Normalization(),\n","        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n","        layers.RandomFlip(\"horizontal\"),\n","        # layers.RandomBrightness(factor=0.2)\n","    ],\n","    name=\"data_augmentation\",\n",")\n","\n","dataset_sample = []\n","k = 0\n","for elem in train_dataset:\n","  if k == 6000:\n","    break\n","  for i in range(len(elem[0])):\n","    k += 1\n","    dataset_sample.append(elem[0][1])\n","data_augmentation.layers[0].adapt(np.array(dataset_sample))\n","del dataset_sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ftU2W9zth2Hp"},"outputs":[],"source":["class ShiftedPatchTokenization(layers.Layer):\n","    def __init__(\n","        self,\n","        image_size=IMAGE_SIZE,\n","        patch_size=PATCH_SIZE,\n","        num_patches=NUM_PATCHES,\n","        projection_dim=PROJECTION_DIM,\n","        vanilla=False,\n","        **kwargs,\n","    ):\n","        super().__init__(**kwargs)\n","        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n","        self.image_size = image_size\n","        self.patch_size = patch_size\n","        self.half_patch = patch_size // 2\n","        self.flatten_patches = layers.Reshape((num_patches, -1))\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n","\n","    def crop_shift_pad(self, images, mode):\n","        # Build the diagonally shifted images\n","        if mode == \"left-up\":\n","            crop_height = self.half_patch\n","            crop_width = self.half_patch\n","            shift_height = 0\n","            shift_width = 0\n","        elif mode == \"left-down\":\n","            crop_height = 0\n","            crop_width = self.half_patch\n","            shift_height = self.half_patch\n","            shift_width = 0\n","        elif mode == \"right-up\":\n","            crop_height = self.half_patch\n","            crop_width = 0\n","            shift_height = 0\n","            shift_width = self.half_patch\n","        else:\n","            crop_height = 0\n","            crop_width = 0\n","            shift_height = self.half_patch\n","            shift_width = self.half_patch\n","\n","        # Crop the shifted images and pad them\n","        crop = tf.image.crop_to_bounding_box(\n","            images,\n","            offset_height=crop_height,\n","            offset_width=crop_width,\n","            target_height=self.image_size - self.half_patch,\n","            target_width=self.image_size - self.half_patch,\n","        )\n","        shift_pad = tf.image.pad_to_bounding_box(\n","            crop,\n","            offset_height=shift_height,\n","            offset_width=shift_width,\n","            target_height=self.image_size,\n","            target_width=self.image_size,\n","        )\n","        return shift_pad\n","\n","    def call(self, images):\n","        if not self.vanilla:\n","            # Concat the shifted images with the original image\n","            images = tf.concat(\n","                [\n","                    images,\n","                    self.crop_shift_pad(images, mode=\"left-up\"),\n","                    self.crop_shift_pad(images, mode=\"left-down\"),\n","                    self.crop_shift_pad(images, mode=\"right-up\"),\n","                    self.crop_shift_pad(images, mode=\"right-down\"),\n","                ],\n","                axis=-1,\n","            )\n","        # Patchify the images and flatten it\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        flat_patches = self.flatten_patches(patches)\n","        if not self.vanilla:\n","            # Layer normalize the flat patches and linearly project it\n","            tokens = self.layer_norm(flat_patches)\n","            tokens = self.projection(tokens)\n","        else:\n","            # Linearly project the flat patches\n","            tokens = self.projection(flat_patches)\n","        return (tokens, patches)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgKwlqB5h5kD"},"outputs":[],"source":["# Get a random image from the training dataset\n","# and resize the image\n","image = next(iter(train_dataset))[0][0] ##x_train[np.random.choice(range(x_train.shape[0]))]\n","resized_image = tf.image.resize(\n","    tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE)\n",")\n","\n","# Vanilla patch maker: This takes an image and divides into\n","# patches as in the original ViT paper\n","(token, patch) = ShiftedPatchTokenization(vanilla=True)(resized_image / 255.0)\n","(token, patch) = (token[0], patch[0])\n","n = patch.shape[0]\n","count = 1\n","plt.figure(figsize=(4, 4))\n","for row in range(n):\n","    for col in range(n):\n","        plt.subplot(n, n, count)\n","        count = count + 1\n","        image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 3))\n","        image = image[::, ::, ::-1]\n","        plt.imshow(image)\n","        plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6KnceNsRiPQg"},"outputs":[],"source":["class PatchEncoder(layers.Layer):\n","    def __init__(\n","        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n","    ):\n","        super().__init__(**kwargs)\n","        self.num_patches = num_patches\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","\n","    def call(self, encoded_patches):\n","        encoded_positions = self.position_embedding(self.positions)\n","        encoded_patches = encoded_patches + encoded_positions\n","        return encoded_patches"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dYT6QRrsiQ4Z"},"outputs":[],"source":["class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n","    def __init__(self, **kwargs):\n","        super().__init__(**kwargs)\n","        # The trainable temperature term. The initial value is\n","        # the square root of the key dimension.\n","        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n","\n","    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n","        query = tf.multiply(query, 1.0 / self.tau)\n","        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n","        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n","        attention_scores_dropout = self._dropout_layer(\n","            attention_scores, training=training\n","        )\n","        attention_output = tf.einsum(\n","            self._combine_equation, attention_scores_dropout, value\n","        )\n","        return attention_output, attention_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a_mibSItiTSs"},"outputs":[],"source":["def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n","\n","\n","# Build the diagonal attention mask\n","diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n","diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNltYyi5iUx0"},"outputs":[],"source":["def create_vit_classifier(vanilla=False):\n","    inputs = layers.Input(shape=INPUT_SHAPE)\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n","    # Encode patches.\n","    encoded_patches = PatchEncoder()(tokens)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(TRANSFORMER_LAYERS):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        if not vanilla:\n","            attention_output = MultiHeadAttentionLSA(\n","                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.3\n","            )(x1, x1, attention_mask=diag_attn_mask)\n","        else:\n","            attention_output = layers.MultiHeadAttention(\n","                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.3\n","            )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.3\n","        )\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n","    # Classify outputs.\n","    logits = layers.Dense(NUM_CLASSES)(features)\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Y2MQvOBiXim"},"outputs":[],"source":["# Some code is taken from:\n","# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n","class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(\n","        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n","    ):\n","        super(WarmUpCosine, self).__init__()\n","\n","        self.learning_rate_base = learning_rate_base\n","        self.total_steps = total_steps\n","        self.warmup_learning_rate = warmup_learning_rate\n","        self.warmup_steps = warmup_steps\n","        self.pi = tf.constant(np.pi)\n","\n","    def __call__(self, step):\n","        if self.total_steps < self.warmup_steps:\n","            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n","\n","        cos_annealed_lr = tf.cos(\n","            self.pi\n","            * (tf.cast(step, tf.float32) - self.warmup_steps)\n","            / float(self.total_steps - self.warmup_steps)\n","        )\n","        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n","\n","        if self.warmup_steps > 0:\n","            if self.learning_rate_base < self.warmup_learning_rate:\n","                raise ValueError(\n","                    \"Learning_rate_base must be larger or equal to \"\n","                    \"warmup_learning_rate.\"\n","                )\n","            slope = (\n","                self.learning_rate_base - self.warmup_learning_rate\n","            ) / self.warmup_steps\n","            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n","            learning_rate = tf.where(\n","                step < self.warmup_steps, warmup_rate, learning_rate\n","            )\n","        return tf.where(\n","            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n","        )\n","\n","\n","def run_experiment(model):\n","    total_steps = int((36060 / BATCH_SIZE) * EPOCHS)\n","    warmup_epoch_percentage = 0.10\n","    warmup_steps = int(total_steps * warmup_epoch_percentage)\n","    scheduled_lrs = WarmUpCosine(\n","        learning_rate_base=LEARNING_RATE,\n","        total_steps=total_steps,\n","        warmup_learning_rate=0.0,\n","        warmup_steps=warmup_steps,\n","    )\n","\n","    optimizer = tfa.optimizers.AdamW(\n","        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n","    )\n","\n","    model.compile(\n","        optimizer=optimizer,\n","        # loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        loss=tf.keras.losses.MeanSquaredError(),\n","        metrics=[\n","            # keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","            # keras.metrics.SparseTopKCategoricalAccuracy(2, name=\"top-2-accuracy\"),\n","            tf.keras.metrics.MeanSquaredError(name=\"MSE\"),\n","            tf.keras.metrics.RootMeanSquaredError(name=\"RMSE\"),\n","        ],\n","    )\n","    model_checkpoint_callback = ModelCheckpoint(\n","        filepath=model_path,\n","        save_weights_only=True,\n","        monitor='val_loss',\n","        mode='min',\n","        save_best_only=True\n","    )\n","\n","    history = model.fit(\n","        train_dataset,\n","        batch_size=BATCH_SIZE,\n","        epochs=EPOCHS,\n","        validation_data=validation_dataset,\n","        callbacks=[model_checkpoint_callback],\n","        shuffle=True\n","    )\n","    # _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n","    # print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n","    # print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n","\n","    return history\n","\n","\n","# Run experiments with the vanilla ViT\n","vit = create_vit_classifier(vanilla=True)\n","history = run_experiment(vit)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzZ0HtKxY6mp"},"outputs":[],"source":["def model_predict(model, x, y, batch_size):\n","  model.compile(\n","        optimizer=\"adam\",\n","        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","        metrics=[\n","            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","            # keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n","        ],\n","    )\n","  model.load_weights(model_path)\n","  model.evaluate(x=x, y=y, batch_size=batch_size)\n","  return model.predict(x)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"RegionGuessrViT.ipynb","provenance":[],"authorship_tag":"ABX9TyPPMWUPdn9UKSQpHeJ3pbfg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}